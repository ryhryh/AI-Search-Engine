{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embed_text: https://cloud.google.com/solutions/machine-learning/analyzing-text-semantic-similarity-using-tensorflow-and-cloud-dataflow?hl=it\n",
    "- entity: https://cloud.google.com/datastore/docs/concepts/entities\n",
    "- filters: https://levelup.gitconnected.com/delete-entities-of-datastore-in-bulk-with-dataflow-implemented-in-python-37cbe2dd7e08\n",
    "- GCP 很貴 在local就好 (DirectRunner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:05:37.562948Z",
     "start_time": "2021-04-05T16:05:37.029315Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import datastore\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:05:37.576856Z",
     "start_time": "2021-04-05T16:05:37.567013Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import datastore\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "class EntityUtil:\n",
    "    def __init__(self, kind='Qa'):\n",
    "        self.date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        self.kind = kind\n",
    "        self.datastore_client = datastore.Client()\n",
    "        self.setup_todo_q()\n",
    "        self.num_tfrecords = math.ceil(len(self.todo_q) / 3)  \n",
    "        \n",
    "    def setup_todo_q(self):\n",
    "        query = self.datastore_client.query(kind=self.kind)\n",
    "        query.add_filter(\"vector_time\", \"=\", 'x')\n",
    "        self.todo_q = list(query.fetch())\n",
    "    \n",
    "    def update_q(self):\n",
    "        done_q = []\n",
    "        for q in self.todo_q:\n",
    "            q['vector_time'] = self.date\n",
    "            done_q.append(q)\n",
    "        self.datastore_client.put_multi(done_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tfrecords():\n",
    "    import gds\n",
    "    entityUtil = gds.EntityUtil()\n",
    "    num_tfrecords = entityUtil.num_tfrecords\n",
    "    return num_tfrecords\n",
    "\n",
    "def process(num_tfrecords):\n",
    "    import dataflow\n",
    "    dataflowUtil = dataflow.DataflowUtil(runner=\"DirectRunner\", num_tfrecords=num_tfrecords)\n",
    "    dataflowUtil.run()\n",
    "    \n",
    "def update_q():\n",
    "    import gds\n",
    "    entityUtil = gds.EntityUtil()\n",
    "    entityUtil.update_q()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    num_tfrecords = get_num_tfrecords()\n",
    "    print(num_tfrecords)\n",
    "    \n",
    "    process(num_tfrecords)\n",
    "    \n",
    "    # 確定生成vec之後 update GDS\n",
    "    print('update_q')\n",
    "    update_q()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:46:42.636086Z",
     "start_time": "2021-04-05T15:46:37.885650Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions\n",
    "from apache_beam.io.gcp.datastore.v1new.datastoreio import ReadFromDatastore, WriteToDatastore\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Entity, Key, Query\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "class DataflowUtil:\n",
    "    def __init__(self, \n",
    "                 num_tfrecords=3,\n",
    "                 project=\"affable-enigma-309308\", \n",
    "                 bucket_name='easy666', \n",
    "                 kind='Qa',\n",
    "                 blob_dirname='Qa',\n",
    "                 runner=\"DataflowRunner\", # DirectRunner\n",
    "                 region=\"us-central1\", \n",
    "                 job_name=\"dataflow_1\", \n",
    "                 machine_type=\"n1-highmem-4\", \n",
    "                 max_num_workers=8,\n",
    "                 setup_file=\"./setup.py\"):\n",
    "        \n",
    "        self.bucket_name = bucket_name\n",
    "        self.blob_dirname = blob_dirname\n",
    "        self.kind = kind\n",
    "        self.runner = runner\n",
    "        self.project = project \n",
    "        self.job_name = job_name\n",
    "        self.region = region\n",
    "        self.machine_type = machine_type\n",
    "        self.max_num_workers = max_num_workers\n",
    "        self.setup_file = setup_file\n",
    "        self.num_tfrecords = num_tfrecords\n",
    "        \n",
    "        self.date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        self.setup_args()\n",
    "        self.setup_pipelineOptions()\n",
    "        self.setup_tft_args()\n",
    "        \n",
    "    def setup_tft_args(self):\n",
    "        self.transform_temp_dir = \"gs://%s/%s/dataflow/transform/temp\"%(self.bucket_name, self.blob_dirname)\n",
    "        self.file_path_prefix = 'gs://%s/%s/embeddings/%s'%(self.bucket_name, self.blob_dirname, self.date)                \n",
    "        self.file_name_suffix='.tfrecords'        \n",
    "            \n",
    "    def setup_args(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        # parser.add_argument()\n",
    "        self.known_args, self.unknown_args = parser.parse_known_args()\n",
    "        \n",
    "    def setup_pipelineOptions(self):\n",
    "        staging_location=\"gs://%s/%s/dataflow/staging\"%(self.bucket_name, self.blob_dirname)\n",
    "        temp_location=\"gs://%s/%s/dataflow/temp\"%(self.bucket_name, self.blob_dirname)\n",
    "        \n",
    "        self.pipelineOptions = PipelineOptions(\n",
    "            staging_location=staging_location, \n",
    "            temp_location=temp_location,\n",
    "            runner=self.runner,\n",
    "            project=self.project,\n",
    "            job_name=self.job_name,\n",
    "            region=self.region, \n",
    "            machine_type=self.machine_type, \n",
    "            max_num_workers=self.max_num_workers, \n",
    "            setup_file=self.setup_file,\n",
    "            flags=self.unknown_args,\n",
    "            save_main_session=True, #!\n",
    "            #streaming=True #!\n",
    "        ) \n",
    "        \n",
    "    def embed_text(self, text):\n",
    "        module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
    "        model = hub.load(module_url)\n",
    "        return model(text)\n",
    "        \n",
    "    def preprocess_fn(self, input_features):\n",
    "        id_ = input_features['id']\n",
    "        text = input_features['text']\n",
    "\n",
    "        embedding = self.embed_text(text)\n",
    "\n",
    "        output_features = {'id': id_,\n",
    "                           'embedding': embedding}\n",
    "        return output_features\n",
    "    \n",
    "    def get_query(self):\n",
    "        filters = [('vector_time', '=', 'x')]\n",
    "        query = Query(\n",
    "            kind=self.kind, \n",
    "            project=self.project, \n",
    "            filters=filters #?\n",
    "        )\n",
    "        return query\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        feature_spec = {}\n",
    "        feature_spec['id'] = tf.io.FixedLenFeature([], tf.string)\n",
    "        feature_spec['text'] = tf.io.FixedLenFeature([], tf.string)\n",
    "\n",
    "        schema = schema_utils.schema_from_feature_spec(feature_spec)\n",
    "        metadata = dataset_metadata.DatasetMetadata(schema)\n",
    "        return metadata\n",
    "\n",
    "    def parse_entity(self, beam_entity):   \n",
    "        datastore_entity = beam_entity.to_client_entity()\n",
    "        name = datastore_entity.key.name\n",
    "        question_text = datastore_entity['question_text']\n",
    "\n",
    "        output = {'id': name,\n",
    "                  'text': question_text}\n",
    "        return output\n",
    "    \n",
    "    def run(self):\n",
    "        query = self.get_query()\n",
    "        p = beam.Pipeline(options=self.pipelineOptions)\n",
    "        x = p | 'read from datastore' >> ReadFromDatastore(query)\n",
    "                \n",
    "        articles = x | \"parse_entity\" >> beam.Map(self.parse_entity)\n",
    "        with tft_beam.Context(temp_dir=self.transform_temp_dir, force_tf_compat_v1=False):\n",
    "            articles_dataset = articles, self.get_metadata()\n",
    "\n",
    "            transformed_dataset, transform_fn = (\n",
    "                articles_dataset | 'Extract embeddings' >> tft_beam.AnalyzeAndTransformDataset(self.preprocess_fn)\n",
    "            )\n",
    "\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "            \n",
    "            transformed_data | 'Write embeddings to TFRecords'  >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                file_path_prefix=self.file_path_prefix,\n",
    "                file_name_suffix=self.file_name_suffix,\n",
    "                coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                num_shards=self.num_tfrecords\n",
    "            )\n",
    "        \n",
    "        result = p.run()        \n",
    "        result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tfrecords = 3\n",
    "dataflowUtil = DataflowUtil(runner=\"DirectRunner\", num_tfrecords=num_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T15:48:06.680925Z",
     "start_time": "2021-04-05T15:46:54.019505Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataflowUtil.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PicklingError: Pickling client objects is explicitly not supported.\n",
    "Clients have non-trivial state that is local and unpickleable.\n",
    "\n",
    "datastore 沒辦法跟 beam 一起執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T14:13:56.814793Z",
     "start_time": "2021-04-05T14:13:56.806467Z"
    }
   },
   "outputs": [],
   "source": [
    "# import setuptools\n",
    "# pkgs = [\n",
    "#     \"tensorflow==2.3.1\",\n",
    "#     \"tensorflow-text==2.3.0\",\n",
    "#     \"tensorflow-transform==0.25.0\",\n",
    "#     \"tensorflow-hub==0.9.0\",\n",
    "#     #\"tfx-bsl==0.25.0\",\n",
    "# ]\n",
    "\n",
    "# setuptools.setup(\n",
    "#     name='dataflow',\n",
    "#     version='0.0',\n",
    "#     install_requires=pkgs,\n",
    "#     packages=setuptools.find_packages(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
