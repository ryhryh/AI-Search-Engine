{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T12:59:41.711963Z",
     "start_time": "2021-04-06T12:59:41.696231Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "import tensorflow as tf\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "class MyClassifier:\n",
    "    def __init__(self, \n",
    "                 vector_length=512, \n",
    "                 metric='angular', \n",
    "                 bucket_name='easy666',\n",
    "                 num_trees=100, \n",
    "                 blob_name='Qa', \n",
    "                 index_path='./index.ann', \n",
    "                 mapping_path='./mapping.pickle'):\n",
    "        \n",
    "        self.vector_length = vector_length\n",
    "        self.metric = metric\n",
    "        self.bucket_name = bucket_name\n",
    "        self.num_trees = num_trees\n",
    "        self.blob_name = blob_name\n",
    "        self.index_path = index_path\n",
    "        self.mapping_path = mapping_path\n",
    "        \n",
    "        self.storage_client = storage.Client()\n",
    "        self.bucket = self.storage_client.bucket(bucket_name)\n",
    "        \n",
    "    def process(self):\n",
    "        self.build_index()\n",
    "        self.upload_file(self.index_path)\n",
    "        self.upload_file(self.mapping_path)\n",
    "        print('done!')\n",
    "        \n",
    "    def build_index(self):\n",
    "        embed_file_list = self.get_embed_file_list()\n",
    "        \n",
    "        mapping = {}\n",
    "        annoy_id = 0\n",
    "        annoy_index = AnnoyIndex(self.vector_length, metric=self.metric)\n",
    "        for i in range(len(embed_file_list)):\n",
    "            embed_file = embed_file_list[i]\n",
    "            record_iterator = tf.compat.v1.python_io.tf_record_iterator(path=embed_file)\n",
    "            for string_record in record_iterator:\n",
    "                example = tf.train.Example()\n",
    "                example.ParseFromString(string_record)\n",
    "                \n",
    "                embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
    "                annoy_index.add_item(annoy_id, embedding)\n",
    "                \n",
    "                gds_name = example.features.feature['id'].bytes_list.value[0]\n",
    "                gds_name = str(gds_name, 'utf-8') \n",
    "                mapping[annoy_id] = gds_name\n",
    "                \n",
    "                annoy_id += 1\n",
    "        annoy_index.build(n_trees=self.num_trees)\n",
    "        annoy_index.save(self.index_path)\n",
    "        annoy_index.unload()\n",
    "        \n",
    "        self.save_mapping(mapping)\n",
    "        \n",
    "    def get_embed_file_list(self):\n",
    "        pattern = \"gs://%s/%s/embeddings/*.tfrecords\"%(self.bucket_name, self.blob_name) # gs://easy826/Question/embeddings/embed-00000-of-00003.tfrecords\n",
    "        embed_file_list = tf.io.gfile.glob(pattern)\n",
    "        return embed_file_list\n",
    "        \n",
    "    def save_mapping(self, mapping):\n",
    "        with open(self.mapping_path, 'wb') as file:\n",
    "            pickle.dump(mapping, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def upload_file(self, file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        blob_name = \"%s/index/%s\"%(self.blob_name, filename)\n",
    "        blob = self.bucket.blob(blob_name)\n",
    "        blob.upload_from_filename(file_path, content_type='application/octet-stream')\n",
    "        \n",
    "myClassifier = MyClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T12:59:42.834885Z",
     "start_time": "2021-04-06T12:59:41.714477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "myClassifier.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T12:58:09.762707Z",
     "start_time": "2021-04-06T12:58:09.759117Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(myClassifier.mapping_path, 'rb') as f:\n",
    "#     mapping = pickle.load(f)\n",
    "# mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T12:58:16.932686Z",
     "start_time": "2021-04-06T12:58:16.919719Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
